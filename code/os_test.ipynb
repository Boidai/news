{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_line_url(url):\n",
    "    line_url = os.popen(\"\"\"curl -XPOST \\\n",
    "    -H \"Authorization: Bearer token\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"view\": {\n",
    "            \"type\": \"tall\",\n",
    "            \"url\": \" '%s' \"\n",
    "        }\n",
    "    }' \\\n",
    "    https://api.line.me/liff/v1/apps\"\"\" % (url)) \n",
    "\n",
    "    my_link = \"line://app/\"+json.loads(line_url.read())['liffId']\n",
    "    line_url.close\n",
    "    return my_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "if_end = True\n",
    "store_datetime = datetime.datetime.now().strftime(\"%Y/%m/%d\")\n",
    "my_json = []\n",
    "for page in range(1, 12):\n",
    "    if if_end == False:\n",
    "        break\n",
    "    href = \"https://tw.news.appledaily.com/politics/realtime/\"+ str(page)\n",
    "    res = requests.get(href)\n",
    "    html = BeautifulSoup(res.text)\n",
    "    all_news_1 = html.find_all(\"ul\", class_=\"rtddd slvl\")\n",
    "    for all_news in all_news_1:\n",
    "        if if_end == False:\n",
    "            break\n",
    "        news = all_news.find_all(\"a\")\n",
    "        for n in news:\n",
    "            my_news = {}\n",
    "            news_url =\"https://tw.news.appledaily.com/\" + str(n[\"href\"])\n",
    "            news_per = requests.get(news_url)\n",
    "            bs = BeautifulSoup(news_per.text)\n",
    "\n",
    "            if bs.find(\"div\", class_=\"ndArticle_view\") == None:\n",
    "                views = 0\n",
    "            else:\n",
    "                views = int(bs.find(\"div\", class_=\"ndArticle_view\").text)\n",
    "            date_time = bs.find(\"div\", class_=\"ndArticle_creat\").text.replace(\"出版時間：\", \"\").split(\" \")[0]\n",
    "            if not date_time == store_datetime:\n",
    "                if_end = False\n",
    "                break\n",
    "            my_news= {\"url\": news_url, \"views\": views}\n",
    "            my_json.append(my_news)\n",
    "            print(my_news)\n",
    "\n",
    "sorted_json = sorted(my_json ,key = lambda my_json:my_json['views'], reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/01/06\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "push_message = datetime.datetime.now().strftime(\"%Y/%m/%d\")\n",
    "print(push_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "ip_location='chatbot_api'\n",
    "\n",
    "@app.route(\"/pushMessage\", methods=['GET'])\n",
    "def push_message():\n",
    "    Endpoint='http://%s:5000/users_id' % (ip_location)\n",
    "\n",
    "    Response=requests.get(Endpoint)\n",
    "    user_id_array = Response.json()\n",
    "    \n",
    "    line_bot_api.multicast(\n",
    "        user_id_array,\n",
    "        imagemap_message\n",
    "    )\n",
    "\n",
    "    return 'OK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gensim\n",
    "import glob\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import jieba\n",
    "\n",
    "\n",
    "def news_push(today_news, most_views_news1, most_views_news2, most_views_news3, most_views_news4):\n",
    "    df_all = pd.DataFrame(columns=[\"title_id\", \"origin_id\", \"title\", \"url\", \"content\"])\n",
    "    for files in glob.glob(today_news):\n",
    "        with open(files, \"r\", encoding=\"utf-8\") as json_data:\n",
    "            data = json.load(json_data)\n",
    "            df = pd.DataFrame.from_dict(data, orient=\"columns\")\n",
    "            df_all = pd.concat([df_all, df], axis=0, ignore_index=True)\n",
    "\n",
    "    df = df_all\n",
    "    df = df.drop([\"url\"], axis=1)\n",
    "    df = df.drop([\"origin_id\"], axis=1)\n",
    "    df = df.drop([\"title\"], axis=1)\n",
    "\n",
    "    def split_news(news):\n",
    "        return \" \".join(jieba.cut(news))\n",
    "\n",
    "    def process_news(df):\n",
    "        df['content'] = df['content'].apply(split_news)\n",
    "        return df\n",
    "\n",
    "    df = process_news(df)\n",
    "\n",
    "    TaggededDocument = gensim.models.doc2vec.TaggedDocument\n",
    "\n",
    "    news_list = []\n",
    "    for i in range(0, len(df)):\n",
    "        news = list(df.iloc[i])\n",
    "        news_list.append(news)\n",
    "\n",
    "    x_train = []\n",
    "    for content, title_id in news_list:\n",
    "        word_list = content.split(' ')\n",
    "        l = len(word_list)\n",
    "        word_list[l - 1] = word_list[l - 1].strip()\n",
    "        document = TaggededDocument(word_list, tags=[title_id])\n",
    "        x_train.append(document)\n",
    "    c = x_train\n",
    "\n",
    "    model = Doc2Vec(x_train, min_count=1, window=3, vector_size=100, negative=5, workers=4)\n",
    "    model.train(x_train, total_examples=model.corpus_count, epochs=10)\n",
    "    model_dm = model\n",
    "\n",
    "    # 第一個推播\n",
    "    strl = df[\"title_id\"] == most_views_news1\n",
    "    news_index = int(str(df[strl][\"content\"]).split(\" \")[0])\n",
    "    test_text = df[strl][\"content\"][news_index].split(' ')\n",
    "    # 得到向量array命名為inferred_vector\n",
    "    inferred_vector = model_dm.infer_vector(doc_words=test_text, alpha=0.025, steps=500)\n",
    "\n",
    "    sims = model_dm.docvecs.most_similar([inferred_vector], topn=3)\n",
    "\n",
    "    title_id_list1 = []\n",
    "    for title_id, sim in sims:\n",
    "        title_id_list1.append(title_id)\n",
    "\n",
    "    # 第二個推播\n",
    "    strl = df[\"title_id\"] == most_views_news2\n",
    "    news_index = int(str(df[strl][\"content\"]).split(\" \")[0])\n",
    "    test_text = df[strl][\"content\"][news_index].split(' ')\n",
    "    # 得到向量array命名為inferred_vector\n",
    "    inferred_vector = model_dm.infer_vector(doc_words=test_text, alpha=0.025, steps=500)\n",
    "\n",
    "    sims = model_dm.docvecs.most_similar([inferred_vector], topn=3)\n",
    "\n",
    "    title_id_list2 = []\n",
    "    for title_id, sim in sims:\n",
    "        title_id_list2.append(title_id)\n",
    "\n",
    "    # 第三個推播\n",
    "    strl = df[\"title_id\"] == most_views_news3\n",
    "    news_index = int(str(df[strl][\"content\"]).split(\" \")[0])\n",
    "    test_text = df[strl][\"content\"][news_index].split(' ')\n",
    "    # 得到向量array命名為inferred_vector\n",
    "    inferred_vector = model_dm.infer_vector(doc_words=test_text, alpha=0.025, steps=500)\n",
    "\n",
    "    sims = model_dm.docvecs.most_similar([inferred_vector], topn=3)\n",
    "\n",
    "    title_id_list3 = []\n",
    "    for title_id, sim in sims:\n",
    "        title_id_list3.append(title_id)\n",
    "\n",
    "    # 第四個推播\n",
    "    strl = df[\"title_id\"] == most_views_news4\n",
    "    news_index = int(str(df[strl][\"content\"]).split(\" \")[0])\n",
    "    test_text = df[strl][\"content\"][news_index].split(' ')\n",
    "    # 得到向量array命名為inferred_vector\n",
    "    inferred_vector = model_dm.infer_vector(doc_words=test_text, alpha=0.025, steps=500)\n",
    "\n",
    "    sims = model_dm.docvecs.most_similar([inferred_vector], topn=3)\n",
    "\n",
    "    title_id_list4 = []\n",
    "    for title_id, sim in sims:\n",
    "        title_id_list4.append(title_id)\n",
    "\n",
    "    return (title_id_list1, title_id_list2, title_id_list3, title_id_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def url_change_titleid(url):\n",
    "    ip_location='chatbot_api'\n",
    "    playload = {\"url\" : url}\n",
    "    Endpoint='http://%s:5000/get_title_id/' % (ip_location)\n",
    "\n",
    "    # header要特別註明是json格式\n",
    "    Header={'Content-Type':'application/json'}\n",
    "\n",
    "    # 傳送post對API server新增資料 \n",
    "    Response=requests.post(Endpoint,headers=Header,data=json.dumps(playload))\n",
    "\n",
    "    #印出Response的資料訊息\n",
    "\n",
    "    return Response.json()['title_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "if_end = True\n",
    "store_datetime = datetime.datetime.now().strftime(\"%Y/%m/%d\")\n",
    "my_json = []\n",
    "for page in range(1, 12):\n",
    "    if if_end == False:\n",
    "        break\n",
    "    href = \"https://tw.news.appledaily.com/politics/realtime/\"+ str(page)\n",
    "    res = requests.get(href)\n",
    "    html = BeautifulSoup(res.text)\n",
    "    all_news_1 = html.find_all(\"ul\", class_=\"rtddd slvl\")\n",
    "    for all_news in all_news_1:\n",
    "        if if_end == False:\n",
    "            break\n",
    "        news = all_news.find_all(\"a\")\n",
    "        for n in news:\n",
    "            my_news = {}\n",
    "            news_url =\"https://tw.news.appledaily.com/\" + str(n[\"href\"])\n",
    "            news_per = requests.get(news_url)\n",
    "            bs = BeautifulSoup(news_per.text)\n",
    "\n",
    "            if bs.find(\"div\", class_=\"ndArticle_view\") == None:\n",
    "                views = 0\n",
    "            else:\n",
    "                views = int(bs.find(\"div\", class_=\"ndArticle_view\").text)\n",
    "            date_time = bs.find(\"div\", class_=\"ndArticle_creat\").text.replace(\"出版時間：\", \"\").split(\" \")[0]\n",
    "            if not date_time == store_datetime:\n",
    "                if_end = False\n",
    "                break\n",
    "            my_news= {\"url\": news_url, \"views\": views}\n",
    "            my_json.append(my_news)\n",
    "            print(my_news)\n",
    "\n",
    "sorted_json = sorted(my_json ,key = lambda my_json:my_json['views'], reverse = True)\n",
    "\n",
    "push_stand_list = []\n",
    "for s in sorted_json[0:4]:\n",
    "    push_stand_list.append(url_change_titleid(s[\"url\"]))    \n",
    "    \n",
    "push_news= news_push(all_news_json, push_stand_list[0], push_stand_list[1], push_stand_list[2], push_stand_list[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
